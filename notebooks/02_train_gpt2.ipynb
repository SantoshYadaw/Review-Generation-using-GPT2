{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6318a276-a163-490f-b9c6-eb0ec4264b93",
   "metadata": {},
   "source": [
    "# 02 - Train GPT2 Model\n",
    "\n",
    "This notebook contains the steps to train open source gpt2 model hosted on hugging face.\n",
    "\n",
    "Author:\n",
    "- Santosh Yadaw\n",
    "- santoshyadawprl@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f2a7e-c4a9-481a-ace5-87985127604f",
   "metadata": {},
   "source": [
    "## a. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2979f0-8144-436a-90ba-60c3d9d8d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, AutoConfig\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60080aa2-27cc-4f7d-bd23-45318941bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2d9b53-0eb8-4294-ae12-ea06a3af385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d135bd1-7f14-42c2-b059-b9de29dcb01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:HOME_PATH: /home/jupyter/text-gen\n",
      "INFO:root:DATA_PATH: /home/jupyter/text-gen/data/raw/task2.csv\n",
      "INFO:root:INTERIM_DATA_PATH: /home/jupyter/text-gen/data/interim/interim_data.csv\n",
      "INFO:root:SPLIT_DATA_PATH: /home/jupyter/text-gen/data/processed/split_data.csv\n",
      "INFO:root:model_path: /home/jupyter/text-gen/models\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "HOME_PATH = os.path.split(os.getcwd())[0]\n",
    "logger.info(f\"HOME_PATH: {HOME_PATH}\")\n",
    "\n",
    "DATA_PATH = os.path.join(HOME_PATH,\"data\", \"raw\", \"task2.csv\")\n",
    "logger.info(f\"DATA_PATH: {DATA_PATH}\")\n",
    "\n",
    "INTERIM_DATA_PATH= os.path.join(HOME_PATH,\"data\", \"interim\", \"interim_data.csv\")\n",
    "logger.info(f\"INTERIM_DATA_PATH: {INTERIM_DATA_PATH}\")\n",
    "\n",
    "SPLIT_DATA_PATH = os.path.join(HOME_PATH,\"data\",\"processed\",\"split_data.csv\")\n",
    "logger.info(f\"SPLIT_DATA_PATH: {SPLIT_DATA_PATH}\")\n",
    "\n",
    "# Set the path to save gpt2 model\n",
    "MODEL_PATH = os.path.join(HOME_PATH, \"models\")\n",
    "logger.info(f\"model_path: {MODEL_PATH}\")\n",
    "\n",
    "# Model training constants\n",
    "TRAIN_SIZE = 0.9\n",
    "SEED = 2023\n",
    "\n",
    "# Specify special tokens for gpt2\n",
    "bos = '<|endoftext|>'\n",
    "eos = '<|EOS|>'\n",
    "pad = '<|pad|>'\n",
    "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad}\n",
    "\n",
    "NUM_EPOCHS_TRAIN=6             # total # of training epochs\n",
    "BATCH_SIZE_TRAIN=1 # batch size per device during training\n",
    "BATCH_SIZE_EVAL=1  # batch size for evaluation\n",
    "WARMUP_STEPS=200              # number of warmup steps for learning rate scheduler\n",
    "WEIGHT_DECAY=0.01              # strength of weight decay\n",
    "LOGGING_DIR=MODEL_PATH           # directory for storing logs\n",
    "PREDICTION_LOSS=True\n",
    "SAVE_STEPS=10000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ead68-a36d-4dd8-8be5-6d2d60a700fd",
   "metadata": {},
   "source": [
    "### Data loading and processing\n",
    "\n",
    "The data processing in this case consists of three steps:\n",
    "\n",
    "1. Remove duplicate since we are gonna split it randomly\n",
    "2. Clean up the dataset\n",
    "3. Train test split\n",
    "4. Add the start and end tokens to the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb46f944-d2c1-40e2-90f2-4b5bf67e508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "def clean(review):\n",
    "    \n",
    "    # Convert to lower case\n",
    "    review = review.lower()\n",
    "    # Remove any numbers\n",
    "    review = re.sub('[^a-z A-Z 0-9-]+', '', review)\n",
    "    # Remove any stopwords in english\n",
    "    review = \" \".join([word for word in review.split() if word not in stopwords.words('english')])\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4612d66e-10f6-416f-b76c-b59b29780631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "data = pd.read_csv(DATA_PATH, encoding=\"ISO-8859-1\")\n",
    "data = data.T.reset_index().T.reset_index(drop=True).rename(columns={0: \"text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34eb7d-fd22-44c2-82c3-3bc5bcfc33a5",
   "metadata": {},
   "source": [
    "#### 1. Remove duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04bf1a5d-a9eb-4dc9-8f0f-9c6cb33f86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['text'].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b11850-6b7d-4912-aefe-d9382941f83b",
   "metadata": {},
   "source": [
    "#### 2. Clean up the text data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce31c11e-85b8-4464-b2d1-71154de1efbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005728244781494141,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 46909,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e99fb3845f942ce91b7f619267a85e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46909 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Remove duplicated and clean up the text data\n",
    "data['text'] = data['text'].progress_apply(clean)\n",
    "data.to_csv(INTERIM_DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9f055-48c3-4364-94fc-4f9679ed9be4",
   "metadata": {},
   "source": [
    "#### 3. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa22682e-f0d0-4253-b15a-5bfe120ed5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42218 samples for training and 4691 for validation\n"
     ]
    }
   ],
   "source": [
    "# Split randomly\n",
    "data_train, data_val = train_test_split(data, train_size = TRAIN_SIZE, random_state = SEED)\n",
    "print(f'There are {len(data_train)} samples for training and {len(data_val)} for validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e62c8bd-3430-499b-b041-21972ed21c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "data_train[\"split\"] = \"train\"\n",
    "data_val[\"split\"] = \"val\"\n",
    "\n",
    "combined_data = pd.concat([data_train, data_val])\n",
    "combined_data.to_csv(SPLIT_DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71187ef1-8038-4472-8d3a-0271bcf7a29a",
   "metadata": {},
   "source": [
    "#### 4. Add start and end tokens to the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95352d1a-c1a2-4adb-9456-b8530cb3a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens\n",
    "data['text'] = bos + ' ' + data['text'] + ' ' + eos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c43e2-470a-4855-916a-dbe00f2e95fd",
   "metadata": {},
   "source": [
    "## Training GPT2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86caef6-0949-476c-aa56-86c7140a1ad1",
   "metadata": {},
   "source": [
    "### Load GPT2 model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2b1636-b171-457b-b764-de1c74460b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Words in vocabulary: 50257\n"
     ]
    }
   ],
   "source": [
    "# Initialise gpt2 tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Checking what is the vocab size for gpt2\n",
    "logger.info(f\"Words in vocabulary: {gpt2_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a91cf0f-1b23-42cf-8cad-4c8236d7b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new token is added to the tokenizer\n",
    "num_added_toks = gpt2_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c89ccec-90c1-4c5e-ad78-4b40a3192061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the model configs and add special tokens\n",
    "config = AutoConfig.from_pretrained('gpt2', \n",
    "                                    bos_token_id=gpt2_tokenizer.bos_token_id,\n",
    "                                    eos_token_id=gpt2_tokenizer.eos_token_id,\n",
    "                                    pad_token_id=gpt2_tokenizer.pad_token_id,\n",
    "                                    output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb02985f-1f03-4b61-9e23-7111d21fd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT2 model with special tokens\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddcb90a4-08a9-49fe-bd69-9f344cca39f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resize embedding to tokenizer dimensions\n",
    "gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8ffb1-5165-4a3a-a362-3657105ff6e9",
   "metadata": {},
   "source": [
    "### Prepare data into format accepted by GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93990f5c-3da9-4571-a63e-66819a35ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def tokenize_function(text: str):\n",
    "    \"\"\"\n",
    "    Tokenize the given text\n",
    "    \"\"\"\n",
    "    return gpt2_tokenizer(text['text'], padding=True, truncation=True, max_length = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d21de1f-aba1-4571-a4c0-81c8c95c57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data using hugging face dataset\n",
    "train_dataset = Dataset.from_pandas(data_train[['text']])\n",
    "val_dataset = Dataset.from_pandas(data_val[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52cb0d97-0e26-4178-ba25-d4a70fff7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9dba2b8820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9dba2b85e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9db972bee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.030305147171020508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 9,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e1955c50f24b2480aec50f4651b913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9dba2b8820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03236031532287598,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 9,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c32e2ebd00d4d0e8dcc4353d8e48998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9dba2b85e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.042340755462646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 9,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff516160dcf244f59118d7f6e3b0ba08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02981114387512207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 9,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a738ab7f434e209d4303957abbced2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03514409065246582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#4",
       "rate": null,
       "total": 9,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86230c4f65a846839141ccb3e0005f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9db972bee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9dba2b8820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9db9704f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9db972bee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f9dba2b8820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028394460678100586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#1",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d700c7a8c2f84f4c9d4cf3cb9b4cb660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03199052810668945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#0",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62409a202781412a8b2961759b6a0c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025003671646118164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#3",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214d3b4879844777b853b7d5a412caae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027760744094848633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#4",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576d2cb70e3144d2868e4e6c04ffaff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04584097862243652,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "#2",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10fc175809846c0b2b2264f54023f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=5,\n",
    "    remove_columns=['text'],\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=5,\n",
    "    remove_columns=['text'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061447d2-d41c-4e0e-ada1-098277ed91d2",
   "metadata": {},
   "source": [
    "### Train GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbacc9dd-2a82-431d-a5b3-a4d5045ac32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training arguments for gpt2. Using default            \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,          # output directory\n",
    "    num_train_epochs=NUM_EPOCHS_TRAIN,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE_TRAIN,  # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE_EVAL,   # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,               # strength of weight decay\n",
    "    logging_dir=MODEL_PATH,            # directory for storing logs\n",
    "    prediction_loss_only=PREDICTION_LOSS,\n",
    "    save_steps=SAVE_STEPS \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e230e86-82b6-4f59-b73f-d5f079f1c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=gpt2_tokenizer,\n",
    "        mlm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3b7e3c3-7894-470e-9e58-5a978bd8bd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='253308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    23/253308 00:02 < 9:19:45, 7.54 it/s, Epoch 0.00/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialise huggingface trainer\n",
    "trainer = Trainer(\n",
    "    model=gpt2_model,                         # instantiated gpt2 model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,         # training dataset\n",
    "    eval_dataset=tokenized_val_dataset            # evaluation dataset\n",
    ")\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bdc9b-de55-4555-856b-e9ba4c3c4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.save_model()\n",
    "gpt2_tokenizer.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed6115-5cf2-4f19-b982-ebc7181632e8",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "text-gen",
   "name": "pytorch-gpu.1-11.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m93"
  },
  "kernelspec": {
   "display_name": "text-gen",
   "language": "python",
   "name": "text-gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
