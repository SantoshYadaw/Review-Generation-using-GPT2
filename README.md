Review Generation using GPT2
==============================

Author: Santosh Yadaw | [LinkedIn](https://www.linkedin.com/in/santosh-yadaw-b32025111/) | [Github](https://github.com/SantoshYadaw/)

Overview
------------

In this project, we aim to train a gpt2 open source model on amazon review dataset.

Project Organization
------------

    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. 
    │
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   └── make_dataset.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │


--------

Environment Setup
------------
1. Build the necessary environment to run the code
```conda env create -f setup.yml```

2. Ensure your have downloaded the dataset from the repo and it is placed under data > raw folder.


Usage via Notebook
------------
Run the notebooks in the order below to perform EDA, Model training and Model evaluation

1. [01_eda.ipynb](notebooks/01_eda.ipynb)
2. [02_train_gpt2.ipynb](notebooks/02_train_gpt2.ipynb)
3. [03_predict_gpt2.ipynb](notebooks/03_predict_gpt2.ipynb)

Usage via Scripts
------------
If you wish to run via the python scripts, just run the main.py script via ``` python main.py ```

1. Preprocess the raw data:
``` python -m src/data/make_dataset ```

2. Train the gpt2 model:
``` python src/models/train_model.py ```

3. Call inference using trained gpt2 model:
``` python src/models/predict_model ```

Exploratory Data Analysis
------------

Exploratory Data Analysis notebook can be found in [01_eda.ipynb](notebooks/01_eda.ipynb). Please refer to the notebook for the details.


Evaluation Metrics
------------
Two evaluation metrics were considered to evaluate the output from the trained gpt2 model against the original given text in the validation dataset.

1. Jaccard Similarity

Jaccard similarity coefficient basically treats the data objects like sets. It is defined as the size of the intersection of two sets divide by the size of the union. We use this as a way to measure how many words that is generated by gpt2 is identical to the original words in the sentence. The higher the ratio means the more similar the words are.

<p align="center">
  <img src="figures/jaccard_similarity.png" width=35%/>
  <br>                  
</p>

2. Word2vec Embeddings Cosine Similarity 

One of the pitfalls of using jaccard similarity is it does not take into account the symatic meaning of the sentences. As language, there are many ways to express things and likewise, certain sentences can the same meaning but can be written in a different way. Hence we can make use of the idea of embedding and calculate the cosine similarity (which is the measure of the similarity between two vectors) between the original and gpt generated text. 

Cosine similarity computes the similarity of two vectors as the cosine of the angle between two vectors. It determines whether two vectors are pointing in roughly the same direction. So if the angle between the vectors is 0 degrees, then the cosine similarity is 1.

To calculate this, we will use a pretrained word2vec model to generate the embeddings of the original text and the gpt2 generated text. Then we will compare the embeddings via cosine similarity.

<p align="center">
  <img src="figures/cosine_similarity.png" width=35%/>
  <br>                  
</p>

Model's Performance
------------



Future Work
------------

References
------------
